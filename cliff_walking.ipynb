{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking: Q Learning vs Sarsa vs Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class Cliff(object):\n",
    "    def __init__(self, grid_x=4, grid_y=12, max_steps=1000):\n",
    "        self.grid_x = grid_x\n",
    "        self.grid_y = grid_y\n",
    "        self.num_states = grid_x * grid_y\n",
    "        self.state_shape = [self.num_states]\n",
    "        self.nb_actions = 4\n",
    "        self.state = np.zeros((self.num_states), dtype='float32')\n",
    "        self.max_steps = max_steps\n",
    "        self.start_pos = [0, 0]\n",
    "        self.pos = np.array(self.start_pos, dtype='int32')\n",
    "        self.goal_state = np.array([self.grid_x - 1, 0], dtype='int32')\n",
    "        self.counter = 0\n",
    "        \n",
    "    def observe(self):\n",
    "        return np.copy(self.state) \n",
    "\n",
    "    def _is_over(self):\n",
    "        # exit or max number of steps\n",
    "        return np.all(self.pos == self.goal_state) or (0 < self.pos[0] < self.grid_x - 1 and self.pos[1] == 0) or \\\n",
    "               self.counter >= self.max_steps \n",
    "\n",
    "    def _update_state(self, action):\n",
    "        # no-op\n",
    "        if (action == 0 and self.pos[0] == 0) or (action == 1 and self.pos[0] == self.grid_x - 1) or \\\n",
    "           (action == 2 and self.pos[1] == 0) or (action == 3 and self.pos[1] == self.grid_y - 1):  \n",
    "            pass\n",
    "        elif action == 0:  # move left\n",
    "            self.state = np.zeros((self.num_states), dtype='float32')\n",
    "            self.state[(self.pos[0] - 1) * self.grid_x + self.pos[1]] = 1\n",
    "            self.pos[0] -= 1\n",
    "        elif action == 1:  # move right\n",
    "            self.state = np.zeros((self.num_states), dtype='float32')\n",
    "            self.state[(self.pos[0] + 1) * self.grid_x + self.pos[1]] = 1\n",
    "            self.pos[0] += 1\n",
    "        elif action == 2:  # move down\n",
    "            self.state = np.zeros((self.num_states), dtype='float32')\n",
    "            self.state[self.pos[0] * self.grid_x + self.pos[1] - 1] = 1\n",
    "            self.pos[1] -= 1\n",
    "        elif action == 3:  # move up\n",
    "            self.state = np.zeros((self.num_states), dtype='float32')\n",
    "            self.state[self.pos[0] * self.grid_x + self.pos[1] + 1] = 1\n",
    "            self.pos[1] += 1\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        if np.all(self.pos == self.goal_state):\n",
    "            return 0\n",
    "        elif  0 < self.pos[0] < self.grid_x - 1 and self.pos[1] == 0:\n",
    "            return -100\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def act(self, action):\n",
    "        assert action == 0 or action == 1 or action == 2 or action == 3\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        self.counter += 1\n",
    "        return np.copy(self.state), reward, game_over\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.zeros((self.num_states), dtype='float32')\n",
    "        self.state[self.start_pos] = 1\n",
    "        self.pos = np.array(self.start_pos, dtype='int32')\n",
    "        self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, state_shape, nb_actions, gamma=0.99, lr=1.0, algo='q', epsilon=0.1):\n",
    "        self.state_shape = state_shape\n",
    "        self.nb_actions = nb_actions\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.algo = algo\n",
    "        self.epsilon = epsilon\n",
    "        self.table = np.zeros((state_shape[0], nb_actions))\n",
    "    \n",
    "    def _predict(self, state):\n",
    "        return self.table[np.argmax(state[0])]\n",
    "    \n",
    "    def _train(self, sarst, a2=None):\n",
    "        s, a, r, s2, t = sarst\n",
    "        q = self.table[np.argmax(s)][a]\n",
    "        if self.algo == 'q':\n",
    "            target = r + (1 - t) * self.gamma * np.max(self.table[np.argmax(s2)])\n",
    "        elif self.algo == 'sarsa':\n",
    "            target = r + (1 - t) * self.gamma * self.table[np.argmax(s2)][a2]\n",
    "        elif self.algo == 'expected_sarsa':\n",
    "            probs = np.ones(self.nb_actions)\n",
    "            probs *= self.epsilon / self.nb_actions\n",
    "            probs[np.argmax(self.table[np.argmax(s2)])] = 1 - self.epsilon\n",
    "            target = r + (1 - t) * self.gamma * np.sum(probs * self.table[np.argmax(s2)])\n",
    "        self.table[np.argmax(s)][a] += self.lr * (target - q)\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        if np.random.binomial(1, epsilon):\n",
    "            return np.random.randint(self.nb_actions)\n",
    "        else:\n",
    "            q = self._predict([state])\n",
    "            return np.argmax(q)\n",
    "\n",
    "    def learn(self, s, a, r, s2, t, a2=None):\n",
    "        return self._train([s, a, r, s2, t], a2=a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "nb_runs_each = 5\n",
    "nb_episodes = 1000\n",
    "epsilon = 0.1\n",
    "algos = ['q', 'sarsa', 'expected_sarsa']\n",
    "\n",
    "env = Cliff()\n",
    "agents = [Agent(env.state_shape, env.nb_actions, algo=algo, epsilon=epsilon) for algo in algos]\n",
    "all_agent_cumul_rewards = np.empty((nb_runs_each, nb_episodes, len(agents)))\n",
    "all_agent_ep_rewards = np.empty((nb_runs_each, nb_episodes, len(agents)))\n",
    "\n",
    "def do_episode(env, agent, epsilon=1.0, learn=True):\n",
    "    env.reset()\n",
    "    terminal = False\n",
    "    reward = 0\n",
    "    actions = []\n",
    "    while not terminal:\n",
    "        s = env.observe()\n",
    "        \n",
    "        a = agent.get_action(s, epsilon=epsilon)\n",
    "        s2, r, terminal = env.act(a)\n",
    "        a2 = None\n",
    "        if agent.algo == 'sarsa':\n",
    "            a2 = agent.get_action(s2, epsilon=epsilon)\n",
    "        if learn:\n",
    "            agent.learn(s, a, r, s2, terminal, a2=a2)\n",
    "        reward += r \n",
    "        actions.append(a)\n",
    "    return reward, actions\n",
    "    \n",
    "for agent_index, agent in enumerate(agents):\n",
    "    for run in range(nb_runs_each):\n",
    "        cumul_rewards = 0\n",
    "        for ep in range(nb_episodes):\n",
    "            ep_reward, _ = do_episode(env, agent, epsilon, learn=True)\n",
    "            cumul_rewards += ep_reward\n",
    "            all_agent_cumul_rewards[run, ep, agent_index] = cumul_rewards\n",
    "            all_agent_ep_rewards[run, ep, agent_index] = ep_reward\n",
    "            \n",
    "with open('cumul_rewards', 'wb') as f:\n",
    "    pickle.dump(all_agent_cumul_rewards, f)\n",
    "with open('ep_rewards', 'wb') as f:\n",
    "    pickle.dump(all_agent_ep_rewards, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "with open('ep_rewards', 'rb') as f:\n",
    "     all_agent_ep_rewards = pickle.load(f)\n",
    "\n",
    "plot = seaborn.tsplot(data=all_agent_ep_rewards, legend=True, condition=algos)\n",
    "plot.legend(loc='upper left', title='Cliff Walking')\n",
    "plt.ylim(-100, 50)\n",
    "plot.set_ylabel(\"Episodic Reward\")\n",
    "plot.set_xlabel(\"Number of Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
